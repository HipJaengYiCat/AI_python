{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EfficientNet_MVtecAD_Dataset",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghee0518/AI_python/blob/main/EfficientNet_b4_by_timm_MVtecAD_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## trina/ test 나눠서 학습"
      ],
      "metadata": {
        "id": "lynhGDxilbRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "torch.hub : Pytorch Hub는 연구 재현성을 촉진하도록 설계된 사전 훈련 된 모델 저장소\n",
        "nvidia_efficientnet_b4 사전 모델 가져옴\n",
        "'''\n",
        "# import torch\n",
        "# model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b4', pretrained=True)\n",
        "!pip install timm\n",
        "import timm\n",
        "model = timm.create_model('efficientnet_b4', pretrained=True, num_classes=88)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZ-ytfDpoKMH",
        "outputId": "2eac1f5f-42fc-4c16-b6d7-221aa42626db"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 28.2 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 14.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b4_ra2_320-7eb33cd5.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_ra2_320-7eb33cd5.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKXv22Yk6zus",
        "outputId": "2820734f-99c9-4089-e461-85bc48ac0030"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 학습 코드\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "#import timm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import transforms, datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "from PIL import Image\n",
        "# from efficientnet_pytorch import EfficientNet\n",
        "# model_name = 'efficientnet-b0'  # b5\n",
        "\n",
        "# image_size = EfficientNet.get_image_size(model_name)\n",
        "# print(image_size)\n",
        "# model = EfficientNet.from_pretrained(model_name, num_classes=88)"
      ],
      "metadata": {
        "id": "Gjs-R_R5lUKg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## transforms 함수 정리\n",
        "* transforms.ToPILImage() - csv 파일로 데이터셋을 받을 경우, PIL image로 바꿔준다.\n",
        "* transforms.CenterCrop(size) - 가운데 부분을 size 크기로 자른다.\n",
        "* transforms.Grayscale(num_output_channels=1) - grayscale로 변환한다.\n",
        "* transforms.RandomAffine(degrees) - 랜덤으로 affine 변형을 한다.\n",
        "* transforms.RandomCrop(size) -이미지를 랜덤으로 아무데나 잘라 size 크기로 출력한다.\n",
        "* transforms.RandomResizedCrop(size) - 이미지 사이즈를 size로 변경한다\n",
        "* transforms.Resize(size) - 이미지 사이즈를 size로 변경한다\n",
        "* transforms.RandomRotation(degrees) 이미지를 랜덤으로 degrees 각도로 회전한다.\n",
        "* transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333)) - 이미지를 랜덤으로 변형한다.\n",
        "* transforms.RandomVerticalFlip(p=0.5) - 이미지를 랜덤으로 수직으로 뒤집는다. p =0이면 뒤집지 않는다.\n",
        "* transforms.RandomHorizontalFlip(p=0.5) - 이미지를 랜덤으로 수평으로 뒤집는다.\n",
        "* transforms.ToTensor() - 이미지 데이터를 tensor로 바꿔준다.\n",
        "* transforms.Normalize(mean, std, inplace=False) - 이미지를 정규화한다."
      ],
      "metadata": {
        "id": "fTgH3bYzN7RA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 데이타 로드!!\n",
        "device = torch.device('cuda')\n",
        "batch_size  = 32\n",
        "random_seed = 1234\n",
        "img_size = 224\n",
        "random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# make dataset\n",
        "#data_path = 'president/president_data'  # class 별 폴더로 나누어진 걸 가져와서 라벨도 달아준다\n",
        "# train_dataset = datasets.ImageFolder(data_path,\n",
        "#                                      transforms.Compose([\n",
        "#                                      transforms.Resize((224, 224)),\n",
        "#                                      transforms.RandomCrop(224),\n",
        "#                                      transforms.RandomRotation(90, expand=True),\n",
        "#                                      transforms.RandomVerticalFlip(),\n",
        "#                                      transforms.RandomHorizontalFlip(),\n",
        "#                                      transforms.ToTensor(), # 이미지 데이터를 tensor로 바꿔준다.\n",
        "#                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])) # Normalize는 많은 이미지의 평균, 표준편차로 정함(보통 많이 하는 값이라고함)\n",
        "\n",
        "# 이미지 경로\n",
        "train_png = sorted(glob('/content/drive/Othercomputers/내 MacBook Pro/open/train/*.png'))\n",
        "label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))} # 오름차순으로 레이블별로 숫자 부여(0부터 시작)\n",
        "\n",
        "\n",
        "## 이미지  > 넘파이 배열로 변환\n",
        "def img_load(path):\n",
        "    img = cv2.imread(path)[:,:,::-1] # Return type:\tnumpy.ndarray / 기본적으로 BGR로 불러옴, RGB 변환함\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    return img\n",
        "\n",
        "train_imgs = [img_load(m) for m in tqdm(train_png)]\n",
        "\n",
        "#train_imgs = np.load('/content/drive/MyDrive/DACON_이상치 탐지 알고리즘 경진대회/train_imgs_224.npy')\n",
        "train_y = pd.read_csv(\"/content/drive/Othercomputers/내 MacBook Pro/open/train_df.csv\")\n",
        "\n",
        "train_labels = train_y[\"label\"] # 레이블순서는 이미지 파일 순서대로임\n",
        "\n",
        "label_unique = sorted(np.unique(train_labels))\n",
        "label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))} # 오름차순으로 레이블별로 숫자 부여(0부터 시작)\n",
        "\n",
        "train_labels = [label_unique[k] for k in train_labels]\n",
        "\n",
        "class Custom_dataset_3(Dataset):\n",
        "    def __init__(self, img_paths, labels, mode='train'):\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.mode=mode\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.img_paths[idx]\n",
        "        # img = Image.fromarray(img)\n",
        "        # img = transforms.Resize((img_size, img_size))(img)\n",
        "        # img = transforms.RandomCrop(img_size)(img)\n",
        "        # img = transforms.RandomRotation(90, expand=False)(img)\n",
        "        # img = transforms.RandomVerticalFlip()(img)\n",
        "        # img = transforms.RandomHorizontalFlip()(img)\n",
        "        img = transforms.ToTensor()(img) # PIL or ndarray -> tensor\n",
        "        img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)\n",
        "        if self.mode=='test':\n",
        "            pass\n",
        "        ## 레이블 > 원-핫 인코딩 하기 \n",
        "        label = self.labels[idx]# (원핫인코딩 안할거면 해당 코드만 필요)\n",
        "        return img, label\n",
        "\n",
        "train_dataset = Custom_dataset_3(np.array(train_imgs), np.array(train_labels), mode='train')\n",
        "\n",
        "### 데이터셋 분리 > 층화추출 & 테스터 데이터 비율 : 0.3, 시드 : 1234, 셔플 = True(default)\n",
        "train_idx, test_idx = train_test_split(list(range(len(train_dataset))), stratify=train_labels, test_size=0.3, random_state=random_seed)\n",
        "\n",
        "datasets = {} # 데이터셋을 담을 딕셔너리\n",
        "datasets['train'] = Subset(train_dataset, train_idx)\n",
        "datasets['test']  = Subset(train_dataset, test_idx)\n",
        "\n",
        "## data loader 선언\n",
        "dataloaders, batch_num = {}, {}\n",
        "dataloaders['train'] = torch.utils.data.DataLoader(datasets['train'],\n",
        "                                                  batch_size=batch_size, shuffle=True,\n",
        "                                                  num_workers=0)\n",
        "dataloaders['test']  = torch.utils.data.DataLoader(datasets['test'],\n",
        "                                                  batch_size=batch_size, shuffle=False,\n",
        "                                                  num_workers=0)\n",
        "'''\n",
        "데이터 변형 코드를 추가한 후 아래 오류코드가 나와\n",
        "Caught TypeError in DataLoader worker process 0\n",
        "DataLoader > num_workers 를 4 -> 0 로 낮춤\n",
        "'''\n",
        "\n",
        "batch_num['train'], batch_num['test'] = len(dataloaders['train']), len(dataloaders['test'])\n",
        "print('batch_size : %d,  train/test(데이터셋개수/배치사이즈) : %d / %d' % (batch_size, batch_num['train'],batch_num['test']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORqFbwLXpSeu",
        "outputId": "13c802a4-d840-420e-b15a-b3bbd8fa1560"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4277/4277 [02:10<00:00, 32.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size : 32,  train/test(데이터셋개수/배치사이즈) : 94 / 41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### f1 스코어 함수 \n",
        "def score_function(real, pred): # 라이브러리 > sklearn.metrics \n",
        "    score = f1_score(real, pred, average=\"macro\")\n",
        "    return score\n",
        "# !pip install torchmetrics\n",
        "# from torchmetrics import F1Score # 라이브러리 > torchmetrics\n",
        "# f1_score = F1Score(num_classes=len(label_unique))\n",
        "\n",
        "# target = torch.tensor([0, 1, 2, 0, 1, 2])\n",
        "# preds = torch.tensor([0, 2, 1, 0, 0, 1])\n",
        "#f1_score(preds, target)"
      ],
      "metadata": {
        "id": "EJ4ZHzQiFUrc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 참고 블로그 : https://keep-steady.tistory.com/35\n",
        "import gc\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    #train_loss, train_acc, valid_loss, valid_acc = [], [], [], []\n",
        "    train_loss, train_acc, valid_loss, valid_acc, train_f1, valid_f1 = [], [], [], [], [], []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))# - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss, running_corrects, num_cnt = 0.0, 0, 0\n",
        "            \n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device) # device = torch.device('cuda')\n",
        "                labels = labels.to(device) # device = torch.device('cuda')\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                num_cnt += len(labels)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "            \n",
        "            epoch_loss = float(running_loss / num_cnt)\n",
        "            epoch_acc  = float((running_corrects.double() / num_cnt).cpu()*100)\n",
        "            f1 = score_function(labels.cpu().data, preds.cpu()) # score_function or f1_score\n",
        "\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "                train_acc.append(epoch_acc)\n",
        "                train_f1.append(f1)\n",
        "            else:\n",
        "                valid_loss.append(epoch_loss)\n",
        "                valid_acc.append(epoch_acc)\n",
        "                valid_f1.append(f1)\n",
        "\n",
        "            print('{} Loss: {:.5f} Acc: {:.5f} macro-f1: {:.5f}'.format(phase, epoch_loss, epoch_acc, f1))\n",
        "           \n",
        "            # deep copy the model(최적모델 저장)\n",
        "            if phase == 'test' and epoch_acc > best_acc:\n",
        "                best_idx = epoch\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "#                 best_model_wts = copy.deepcopy(model.module.state_dict())\n",
        "                print('==> best model saved - %d / %.1f'%(best_idx, best_acc))\n",
        "        # 한 에포크마다 실행 시간 출력하기(누적 시간으로 출력됨)\n",
        "        time_elapsed = time.time() - since\n",
        "        print('one epochs training time : {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "        print('\\n')\n",
        "        # 한 에포크마다 필요없는 메모리 지우기 : 지우기 효과는 아직 확인 못해봄\n",
        "        try:      \n",
        "          gc.collect() # cpu 비움\n",
        "          torch.cuda.empty_cache() # gpu 비움\n",
        "        except:\n",
        "          pass\n",
        "    ## 학습 마무리 후\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best valid Acc: %d - %.1f' %(best_idx, best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    '''\n",
        "    한 에포크 마다 저장된 최적의 모델 가중치로 조정 후 다음 에포크가 돌아감\n",
        "    '''\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    torch.save(model.state_dict(), 'president_model.pt')\n",
        "    print('model saved')\n",
        "    return model, best_idx, best_acc, train_loss, train_acc, valid_loss, valid_acc, train_f1, valid_f1\n",
        "\n",
        "# 설정\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # set gpu\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_ft = optim.SGD(model.parameters(), \n",
        "                         lr = 0.05,\n",
        "                         momentum=0.9,\n",
        "                         weight_decay=1e-4)\n",
        "\n",
        "lmbda = lambda epoch: 0.98739\n",
        "exp_lr_scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer_ft, lr_lambda=lmbda)\n",
        "\n",
        "# 사전학습된 가중치와 모델을 가져와 데이터셋으로 추가 모델 학습\n",
        "model, best_idx, best_acc, train_loss, train_acc, valid_loss, valid_acc, train_f1, valid_f1 = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDNj4Yarkx-R",
        "outputId": "f96ecb61-9ec0-4e16-f29e-d9d724041ffe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/100\n",
            "----------\n",
            "train Loss: 1.34288 Acc: 79.48547 macro-f1: 0.73810\n",
            "test Loss: 0.72920 Acc: 85.28037 macro-f1: 1.00000\n",
            "==> best model saved - 0 / 85.3\n",
            "one epochs training time : 0m 50s\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "----------\n",
            "train Loss: 0.57053 Acc: 86.10090 macro-f1: 0.87879\n",
            "test Loss: 0.50324 Acc: 87.22741 macro-f1: 1.00000\n",
            "==> best model saved - 1 / 87.2\n",
            "one epochs training time : 2m 10s\n",
            "\n",
            "\n",
            "Epoch 2/100\n",
            "----------\n",
            "train Loss: 0.38935 Acc: 89.67591 macro-f1: 0.74510\n",
            "test Loss: 0.42871 Acc: 88.78505 macro-f1: 1.00000\n",
            "==> best model saved - 2 / 88.8\n",
            "one epochs training time : 3m 1s\n",
            "\n",
            "\n",
            "Epoch 3/100\n",
            "----------\n",
            "train Loss: 0.26805 Acc: 92.71634 macro-f1: 0.80513\n",
            "test Loss: 0.37435 Acc: 90.26480 macro-f1: 1.00000\n",
            "==> best model saved - 3 / 90.3\n",
            "one epochs training time : 3m 52s\n",
            "\n",
            "\n",
            "Epoch 4/100\n",
            "----------\n",
            "train Loss: 0.20918 Acc: 94.08620 macro-f1: 1.00000\n",
            "test Loss: 0.29911 Acc: 91.12150 macro-f1: 1.00000\n",
            "==> best model saved - 4 / 91.1\n",
            "one epochs training time : 4m 43s\n",
            "\n",
            "\n",
            "Epoch 5/100\n",
            "----------\n",
            "train Loss: 0.12682 Acc: 96.32476 macro-f1: 1.00000\n",
            "test Loss: 0.28495 Acc: 91.74455 macro-f1: 1.00000\n",
            "==> best model saved - 5 / 91.7\n",
            "one epochs training time : 5m 34s\n",
            "\n",
            "\n",
            "Epoch 6/100\n",
            "----------\n",
            "train Loss: 0.08450 Acc: 97.69462 macro-f1: 1.00000\n",
            "test Loss: 0.27587 Acc: 91.74455 macro-f1: 1.00000\n",
            "one epochs training time : 6m 24s\n",
            "\n",
            "\n",
            "Epoch 7/100\n",
            "----------\n",
            "train Loss: 0.06129 Acc: 98.39626 macro-f1: 1.00000\n",
            "test Loss: 0.26722 Acc: 93.53583 macro-f1: 1.00000\n",
            "==> best model saved - 7 / 93.5\n",
            "one epochs training time : 7m 15s\n",
            "\n",
            "\n",
            "Epoch 8/100\n",
            "----------\n",
            "train Loss: 0.03904 Acc: 98.89743 macro-f1: 0.88889\n",
            "test Loss: 0.27386 Acc: 92.67913 macro-f1: 1.00000\n",
            "one epochs training time : 8m 6s\n",
            "\n",
            "\n",
            "Epoch 9/100\n",
            "----------\n",
            "train Loss: 0.03330 Acc: 99.26495 macro-f1: 1.00000\n",
            "test Loss: 0.31219 Acc: 92.83489 macro-f1: 1.00000\n",
            "one epochs training time : 8m 57s\n",
            "\n",
            "\n",
            "Epoch 10/100\n",
            "----------\n",
            "train Loss: 0.02715 Acc: 99.23154 macro-f1: 0.81818\n",
            "test Loss: 0.27430 Acc: 93.14642 macro-f1: 1.00000\n",
            "one epochs training time : 9m 47s\n",
            "\n",
            "\n",
            "Epoch 11/100\n",
            "----------\n",
            "train Loss: 0.02472 Acc: 99.29836 macro-f1: 0.61905\n",
            "test Loss: 0.25785 Acc: 94.00312 macro-f1: 1.00000\n",
            "==> best model saved - 11 / 94.0\n",
            "one epochs training time : 10m 38s\n",
            "\n",
            "\n",
            "Epoch 12/100\n",
            "----------\n",
            "train Loss: 0.01251 Acc: 99.76612 macro-f1: 1.00000\n",
            "test Loss: 0.25613 Acc: 93.84735 macro-f1: 1.00000\n",
            "one epochs training time : 11m 29s\n",
            "\n",
            "\n",
            "Epoch 13/100\n",
            "----------\n",
            "train Loss: 0.01077 Acc: 99.73271 macro-f1: 1.00000\n",
            "test Loss: 0.29966 Acc: 93.53583 macro-f1: 1.00000\n",
            "one epochs training time : 12m 20s\n",
            "\n",
            "\n",
            "Epoch 14/100\n",
            "----------\n",
            "train Loss: 0.00758 Acc: 99.83294 macro-f1: 1.00000\n",
            "test Loss: 0.28047 Acc: 93.92523 macro-f1: 1.00000\n",
            "one epochs training time : 13m 10s\n",
            "\n",
            "\n",
            "Epoch 15/100\n",
            "----------\n",
            "train Loss: 0.00534 Acc: 99.93318 macro-f1: 1.00000\n",
            "test Loss: 0.28078 Acc: 94.39252 macro-f1: 1.00000\n",
            "==> best model saved - 15 / 94.4\n",
            "one epochs training time : 14m 1s\n",
            "\n",
            "\n",
            "Epoch 16/100\n",
            "----------\n",
            "train Loss: 0.00264 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.27978 Acc: 93.84735 macro-f1: 1.00000\n",
            "one epochs training time : 14m 52s\n",
            "\n",
            "\n",
            "Epoch 17/100\n",
            "----------\n",
            "train Loss: 0.00376 Acc: 99.93318 macro-f1: 1.00000\n",
            "test Loss: 0.37788 Acc: 92.83489 macro-f1: 1.00000\n",
            "one epochs training time : 15m 43s\n",
            "\n",
            "\n",
            "Epoch 18/100\n",
            "----------\n",
            "train Loss: 0.00472 Acc: 99.93318 macro-f1: 0.86667\n",
            "test Loss: 0.30500 Acc: 93.84735 macro-f1: 1.00000\n",
            "one epochs training time : 16m 33s\n",
            "\n",
            "\n",
            "Epoch 19/100\n",
            "----------\n",
            "train Loss: 0.00555 Acc: 99.83294 macro-f1: 1.00000\n",
            "test Loss: 0.32225 Acc: 93.69159 macro-f1: 1.00000\n",
            "one epochs training time : 17m 24s\n",
            "\n",
            "\n",
            "Epoch 20/100\n",
            "----------\n",
            "train Loss: 0.00173 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.29756 Acc: 94.39252 macro-f1: 1.00000\n",
            "one epochs training time : 18m 15s\n",
            "\n",
            "\n",
            "Epoch 21/100\n",
            "----------\n",
            "train Loss: 0.00408 Acc: 99.89977 macro-f1: 1.00000\n",
            "test Loss: 0.31961 Acc: 93.84735 macro-f1: 1.00000\n",
            "one epochs training time : 19m 6s\n",
            "\n",
            "\n",
            "Epoch 22/100\n",
            "----------\n",
            "train Loss: 0.00190 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.32286 Acc: 94.08100 macro-f1: 1.00000\n",
            "one epochs training time : 19m 56s\n",
            "\n",
            "\n",
            "Epoch 23/100\n",
            "----------\n",
            "train Loss: 0.00237 Acc: 99.93318 macro-f1: 0.81818\n",
            "test Loss: 0.33899 Acc: 94.00312 macro-f1: 1.00000\n",
            "one epochs training time : 20m 47s\n",
            "\n",
            "\n",
            "Epoch 24/100\n",
            "----------\n",
            "train Loss: 0.00827 Acc: 99.76612 macro-f1: 1.00000\n",
            "test Loss: 0.31680 Acc: 93.30218 macro-f1: 1.00000\n",
            "one epochs training time : 21m 38s\n",
            "\n",
            "\n",
            "Epoch 25/100\n",
            "----------\n",
            "train Loss: 0.00600 Acc: 99.83294 macro-f1: 0.87222\n",
            "test Loss: 0.36000 Acc: 93.92523 macro-f1: 1.00000\n",
            "one epochs training time : 22m 29s\n",
            "\n",
            "\n",
            "Epoch 26/100\n",
            "----------\n",
            "train Loss: 0.00821 Acc: 99.73271 macro-f1: 1.00000\n",
            "test Loss: 0.28232 Acc: 94.31464 macro-f1: 1.00000\n",
            "one epochs training time : 23m 19s\n",
            "\n",
            "\n",
            "Epoch 27/100\n",
            "----------\n",
            "train Loss: 0.00560 Acc: 99.83294 macro-f1: 0.81818\n",
            "test Loss: 0.27722 Acc: 94.31464 macro-f1: 1.00000\n",
            "one epochs training time : 24m 10s\n",
            "\n",
            "\n",
            "Epoch 28/100\n",
            "----------\n",
            "train Loss: 0.00649 Acc: 99.86635 macro-f1: 1.00000\n",
            "test Loss: 0.27668 Acc: 94.39252 macro-f1: 1.00000\n",
            "one epochs training time : 25m 1s\n",
            "\n",
            "\n",
            "Epoch 29/100\n",
            "----------\n",
            "train Loss: 0.00188 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.27821 Acc: 94.00312 macro-f1: 1.00000\n",
            "one epochs training time : 25m 52s\n",
            "\n",
            "\n",
            "Epoch 30/100\n",
            "----------\n",
            "train Loss: 0.00194 Acc: 99.96659 macro-f1: 1.00000\n",
            "test Loss: 0.30433 Acc: 94.23676 macro-f1: 1.00000\n",
            "one epochs training time : 26m 42s\n",
            "\n",
            "\n",
            "Epoch 31/100\n",
            "----------\n",
            "train Loss: 0.00126 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.31417 Acc: 93.84735 macro-f1: 1.00000\n",
            "one epochs training time : 27m 33s\n",
            "\n",
            "\n",
            "Epoch 32/100\n",
            "----------\n",
            "train Loss: 0.00101 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.30379 Acc: 94.08100 macro-f1: 1.00000\n",
            "one epochs training time : 28m 24s\n",
            "\n",
            "\n",
            "Epoch 33/100\n",
            "----------\n",
            "train Loss: 0.00106 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.30136 Acc: 94.00312 macro-f1: 1.00000\n",
            "one epochs training time : 29m 15s\n",
            "\n",
            "\n",
            "Epoch 34/100\n",
            "----------\n",
            "train Loss: 0.00064 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.30698 Acc: 94.78193 macro-f1: 1.00000\n",
            "==> best model saved - 34 / 94.8\n",
            "one epochs training time : 30m 6s\n",
            "\n",
            "\n",
            "Epoch 35/100\n",
            "----------\n",
            "train Loss: 0.00090 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.32307 Acc: 94.54829 macro-f1: 1.00000\n",
            "one epochs training time : 30m 57s\n",
            "\n",
            "\n",
            "Epoch 36/100\n",
            "----------\n",
            "train Loss: 0.00111 Acc: 99.96659 macro-f1: 1.00000\n",
            "test Loss: 0.30994 Acc: 94.39252 macro-f1: 1.00000\n",
            "one epochs training time : 31m 48s\n",
            "\n",
            "\n",
            "Epoch 37/100\n",
            "----------\n",
            "train Loss: 0.00084 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.32401 Acc: 94.62617 macro-f1: 1.00000\n",
            "one epochs training time : 32m 39s\n",
            "\n",
            "\n",
            "Epoch 38/100\n",
            "----------\n",
            "train Loss: 0.00129 Acc: 99.96659 macro-f1: 1.00000\n",
            "test Loss: 0.29429 Acc: 94.31464 macro-f1: 1.00000\n",
            "one epochs training time : 33m 30s\n",
            "\n",
            "\n",
            "Epoch 39/100\n",
            "----------\n",
            "train Loss: 0.00056 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.30711 Acc: 94.23676 macro-f1: 1.00000\n",
            "one epochs training time : 34m 21s\n",
            "\n",
            "\n",
            "Epoch 40/100\n",
            "----------\n",
            "train Loss: 0.00124 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.31586 Acc: 94.39252 macro-f1: 1.00000\n",
            "one epochs training time : 35m 12s\n",
            "\n",
            "\n",
            "Epoch 41/100\n",
            "----------\n",
            "train Loss: 0.00122 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.32223 Acc: 94.39252 macro-f1: 1.00000\n",
            "one epochs training time : 36m 3s\n",
            "\n",
            "\n",
            "Epoch 42/100\n",
            "----------\n",
            "train Loss: 0.00051 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.32762 Acc: 94.54829 macro-f1: 1.00000\n",
            "one epochs training time : 36m 54s\n",
            "\n",
            "\n",
            "Epoch 43/100\n",
            "----------\n",
            "train Loss: 0.00133 Acc: 99.93318 macro-f1: 0.88000\n",
            "test Loss: 0.34075 Acc: 94.23676 macro-f1: 1.00000\n",
            "one epochs training time : 37m 45s\n",
            "\n",
            "\n",
            "Epoch 44/100\n",
            "----------\n",
            "train Loss: 0.00469 Acc: 99.83294 macro-f1: 1.00000\n",
            "test Loss: 0.34718 Acc: 93.76947 macro-f1: 1.00000\n",
            "one epochs training time : 38m 36s\n",
            "\n",
            "\n",
            "Epoch 45/100\n",
            "----------\n",
            "train Loss: 0.00624 Acc: 99.96659 macro-f1: 1.00000\n",
            "test Loss: 0.31546 Acc: 94.31464 macro-f1: 1.00000\n",
            "one epochs training time : 39m 27s\n",
            "\n",
            "\n",
            "Epoch 46/100\n",
            "----------\n",
            "train Loss: 0.00073 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.31989 Acc: 94.08100 macro-f1: 1.00000\n",
            "one epochs training time : 40m 18s\n",
            "\n",
            "\n",
            "Epoch 47/100\n",
            "----------\n",
            "train Loss: 0.00104 Acc: 99.96659 macro-f1: 1.00000\n",
            "test Loss: 0.32573 Acc: 94.31464 macro-f1: 1.00000\n",
            "one epochs training time : 41m 8s\n",
            "\n",
            "\n",
            "Epoch 48/100\n",
            "----------\n",
            "train Loss: 0.00070 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.30982 Acc: 94.62617 macro-f1: 1.00000\n",
            "one epochs training time : 41m 59s\n",
            "\n",
            "\n",
            "Epoch 49/100\n",
            "----------\n",
            "train Loss: 0.00084 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.31392 Acc: 94.70405 macro-f1: 1.00000\n",
            "one epochs training time : 42m 50s\n",
            "\n",
            "\n",
            "Epoch 50/100\n",
            "----------\n",
            "train Loss: 0.00056 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.31195 Acc: 94.23676 macro-f1: 1.00000\n",
            "one epochs training time : 43m 41s\n",
            "\n",
            "\n",
            "Epoch 51/100\n",
            "----------\n",
            "train Loss: 0.00055 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.32349 Acc: 94.54829 macro-f1: 1.00000\n",
            "one epochs training time : 44m 32s\n",
            "\n",
            "\n",
            "Epoch 52/100\n",
            "----------\n",
            "train Loss: 0.00114 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.31537 Acc: 94.78193 macro-f1: 1.00000\n",
            "one epochs training time : 45m 23s\n",
            "\n",
            "\n",
            "Epoch 53/100\n",
            "----------\n",
            "train Loss: 0.00045 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.31277 Acc: 94.23676 macro-f1: 1.00000\n",
            "one epochs training time : 46m 14s\n",
            "\n",
            "\n",
            "Epoch 54/100\n",
            "----------\n",
            "train Loss: 0.00050 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.32240 Acc: 94.31464 macro-f1: 1.00000\n",
            "one epochs training time : 47m 5s\n",
            "\n",
            "\n",
            "Epoch 55/100\n",
            "----------\n",
            "train Loss: 0.00052 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.32124 Acc: 94.54829 macro-f1: 1.00000\n",
            "one epochs training time : 47m 56s\n",
            "\n",
            "\n",
            "Epoch 56/100\n",
            "----------\n",
            "train Loss: 0.00098 Acc: 99.96659 macro-f1: 1.00000\n",
            "test Loss: 0.33428 Acc: 94.47040 macro-f1: 1.00000\n",
            "one epochs training time : 48m 47s\n",
            "\n",
            "\n",
            "Epoch 57/100\n",
            "----------\n",
            "train Loss: 0.00104 Acc: 99.96659 macro-f1: 1.00000\n",
            "test Loss: 0.32877 Acc: 94.62617 macro-f1: 1.00000\n",
            "one epochs training time : 49m 38s\n",
            "\n",
            "\n",
            "Epoch 58/100\n",
            "----------\n",
            "train Loss: 0.00067 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.33346 Acc: 94.62617 macro-f1: 1.00000\n",
            "one epochs training time : 50m 30s\n",
            "\n",
            "\n",
            "Epoch 59/100\n",
            "----------\n",
            "train Loss: 0.00071 Acc: 99.96659 macro-f1: 0.81818\n",
            "test Loss: 0.32378 Acc: 94.39252 macro-f1: 1.00000\n",
            "one epochs training time : 51m 21s\n",
            "\n",
            "\n",
            "Epoch 60/100\n",
            "----------\n",
            "train Loss: 0.00087 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.36694 Acc: 93.84735 macro-f1: 1.00000\n",
            "one epochs training time : 52m 13s\n",
            "\n",
            "\n",
            "Epoch 61/100\n",
            "----------\n",
            "train Loss: 0.00044 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.34027 Acc: 94.23676 macro-f1: 1.00000\n",
            "one epochs training time : 53m 4s\n",
            "\n",
            "\n",
            "Epoch 62/100\n",
            "----------\n",
            "train Loss: 0.00045 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.33998 Acc: 94.23676 macro-f1: 1.00000\n",
            "one epochs training time : 53m 55s\n",
            "\n",
            "\n",
            "Epoch 63/100\n",
            "----------\n",
            "train Loss: 0.00024 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.34678 Acc: 94.23676 macro-f1: 1.00000\n",
            "one epochs training time : 54m 46s\n",
            "\n",
            "\n",
            "Epoch 64/100\n",
            "----------\n",
            "train Loss: 0.00074 Acc: 99.96659 macro-f1: 1.00000\n",
            "test Loss: 0.34045 Acc: 94.62617 macro-f1: 1.00000\n",
            "one epochs training time : 55m 37s\n",
            "\n",
            "\n",
            "Epoch 65/100\n",
            "----------\n",
            "train Loss: 0.00032 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.34087 Acc: 94.31464 macro-f1: 1.00000\n",
            "one epochs training time : 56m 28s\n",
            "\n",
            "\n",
            "Epoch 66/100\n",
            "----------\n",
            "train Loss: 0.00026 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.34585 Acc: 94.39252 macro-f1: 1.00000\n",
            "one epochs training time : 57m 19s\n",
            "\n",
            "\n",
            "Epoch 67/100\n",
            "----------\n",
            "train Loss: 0.00034 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.35625 Acc: 94.15888 macro-f1: 1.00000\n",
            "one epochs training time : 58m 10s\n",
            "\n",
            "\n",
            "Epoch 68/100\n",
            "----------\n",
            "train Loss: 0.00025 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.35492 Acc: 94.15888 macro-f1: 1.00000\n",
            "one epochs training time : 59m 1s\n",
            "\n",
            "\n",
            "Epoch 69/100\n",
            "----------\n",
            "train Loss: 0.00021 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.35589 Acc: 94.08100 macro-f1: 1.00000\n",
            "one epochs training time : 59m 52s\n",
            "\n",
            "\n",
            "Epoch 70/100\n",
            "----------\n",
            "train Loss: 0.00042 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.33904 Acc: 94.15888 macro-f1: 1.00000\n",
            "one epochs training time : 60m 43s\n",
            "\n",
            "\n",
            "Epoch 71/100\n",
            "----------\n",
            "train Loss: 0.00028 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.33613 Acc: 93.92523 macro-f1: 1.00000\n",
            "one epochs training time : 61m 35s\n",
            "\n",
            "\n",
            "Epoch 72/100\n",
            "----------\n",
            "train Loss: 0.00072 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.35811 Acc: 94.47040 macro-f1: 1.00000\n",
            "one epochs training time : 62m 25s\n",
            "\n",
            "\n",
            "Epoch 73/100\n",
            "----------\n",
            "train Loss: 0.00026 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.34967 Acc: 94.08100 macro-f1: 1.00000\n",
            "one epochs training time : 63m 16s\n",
            "\n",
            "\n",
            "Epoch 74/100\n",
            "----------\n",
            "train Loss: 0.00021 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.35988 Acc: 94.15888 macro-f1: 1.00000\n",
            "one epochs training time : 64m 8s\n",
            "\n",
            "\n",
            "Epoch 75/100\n",
            "----------\n",
            "train Loss: 0.00042 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.35261 Acc: 94.23676 macro-f1: 1.00000\n",
            "one epochs training time : 64m 59s\n",
            "\n",
            "\n",
            "Epoch 76/100\n",
            "----------\n",
            "train Loss: 0.00031 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.36006 Acc: 94.31464 macro-f1: 1.00000\n",
            "one epochs training time : 65m 50s\n",
            "\n",
            "\n",
            "Epoch 77/100\n",
            "----------\n",
            "train Loss: 0.00028 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.37010 Acc: 94.23676 macro-f1: 1.00000\n",
            "one epochs training time : 66m 42s\n",
            "\n",
            "\n",
            "Epoch 78/100\n",
            "----------\n",
            "train Loss: 0.00113 Acc: 99.93318 macro-f1: 1.00000\n",
            "test Loss: 0.40298 Acc: 93.92523 macro-f1: 1.00000\n",
            "one epochs training time : 67m 33s\n",
            "\n",
            "\n",
            "Epoch 79/100\n",
            "----------\n",
            "train Loss: 0.00162 Acc: 99.93318 macro-f1: 1.00000\n",
            "test Loss: 0.39201 Acc: 93.69159 macro-f1: 1.00000\n",
            "one epochs training time : 68m 24s\n",
            "\n",
            "\n",
            "Epoch 80/100\n",
            "----------\n",
            "train Loss: 0.00130 Acc: 99.93318 macro-f1: 1.00000\n",
            "test Loss: 0.35540 Acc: 93.92523 macro-f1: 1.00000\n",
            "one epochs training time : 69m 15s\n",
            "\n",
            "\n",
            "Epoch 81/100\n",
            "----------\n",
            "train Loss: 0.00059 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.33613 Acc: 94.54829 macro-f1: 1.00000\n",
            "one epochs training time : 70m 7s\n",
            "\n",
            "\n",
            "Epoch 82/100\n",
            "----------\n",
            "train Loss: 0.00025 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.34953 Acc: 94.15888 macro-f1: 1.00000\n",
            "one epochs training time : 70m 58s\n",
            "\n",
            "\n",
            "Epoch 83/100\n",
            "----------\n",
            "train Loss: 0.00055 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.35409 Acc: 94.23676 macro-f1: 1.00000\n",
            "one epochs training time : 71m 49s\n",
            "\n",
            "\n",
            "Epoch 84/100\n",
            "----------\n",
            "train Loss: 0.00033 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.35508 Acc: 94.39252 macro-f1: 1.00000\n",
            "one epochs training time : 72m 41s\n",
            "\n",
            "\n",
            "Epoch 85/100\n",
            "----------\n",
            "train Loss: 0.00217 Acc: 99.96659 macro-f1: 0.89744\n",
            "test Loss: 0.36943 Acc: 94.08100 macro-f1: 1.00000\n",
            "one epochs training time : 73m 32s\n",
            "\n",
            "\n",
            "Epoch 86/100\n",
            "----------\n",
            "train Loss: 0.00117 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.36112 Acc: 94.39252 macro-f1: 1.00000\n",
            "one epochs training time : 74m 23s\n",
            "\n",
            "\n",
            "Epoch 87/100\n",
            "----------\n",
            "train Loss: 0.00180 Acc: 99.89977 macro-f1: 1.00000\n",
            "test Loss: 0.36871 Acc: 94.23676 macro-f1: 1.00000\n",
            "one epochs training time : 75m 14s\n",
            "\n",
            "\n",
            "Epoch 88/100\n",
            "----------\n",
            "train Loss: 0.00093 Acc: 99.96659 macro-f1: 1.00000\n",
            "test Loss: 0.34982 Acc: 94.31464 macro-f1: 1.00000\n",
            "one epochs training time : 76m 5s\n",
            "\n",
            "\n",
            "Epoch 89/100\n",
            "----------\n",
            "train Loss: 0.00069 Acc: 99.96659 macro-f1: 1.00000\n",
            "test Loss: 0.31222 Acc: 94.31464 macro-f1: 1.00000\n",
            "one epochs training time : 76m 56s\n",
            "\n",
            "\n",
            "Epoch 90/100\n",
            "----------\n",
            "train Loss: 0.00190 Acc: 99.89977 macro-f1: 0.67619\n",
            "test Loss: 0.33617 Acc: 94.00312 macro-f1: 1.00000\n",
            "one epochs training time : 77m 47s\n",
            "\n",
            "\n",
            "Epoch 91/100\n",
            "----------\n",
            "train Loss: 0.00224 Acc: 99.96659 macro-f1: 1.00000\n",
            "test Loss: 0.33443 Acc: 94.31464 macro-f1: 1.00000\n",
            "one epochs training time : 78m 38s\n",
            "\n",
            "\n",
            "Epoch 92/100\n",
            "----------\n",
            "train Loss: 0.00025 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.35243 Acc: 94.23676 macro-f1: 1.00000\n",
            "one epochs training time : 79m 29s\n",
            "\n",
            "\n",
            "Epoch 93/100\n",
            "----------\n",
            "train Loss: 0.00026 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.33442 Acc: 94.47040 macro-f1: 1.00000\n",
            "one epochs training time : 80m 20s\n",
            "\n",
            "\n",
            "Epoch 94/100\n",
            "----------\n",
            "train Loss: 0.00064 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.33548 Acc: 94.39252 macro-f1: 1.00000\n",
            "one epochs training time : 81m 11s\n",
            "\n",
            "\n",
            "Epoch 95/100\n",
            "----------\n",
            "train Loss: 0.00038 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.33664 Acc: 94.23676 macro-f1: 1.00000\n",
            "one epochs training time : 82m 2s\n",
            "\n",
            "\n",
            "Epoch 96/100\n",
            "----------\n",
            "train Loss: 0.00030 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.33957 Acc: 94.70405 macro-f1: 1.00000\n",
            "one epochs training time : 82m 52s\n",
            "\n",
            "\n",
            "Epoch 97/100\n",
            "----------\n",
            "train Loss: 0.00024 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.33703 Acc: 94.54829 macro-f1: 1.00000\n",
            "one epochs training time : 83m 43s\n",
            "\n",
            "\n",
            "Epoch 98/100\n",
            "----------\n",
            "train Loss: 0.00026 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.33258 Acc: 94.47040 macro-f1: 1.00000\n",
            "one epochs training time : 84m 34s\n",
            "\n",
            "\n",
            "Epoch 99/100\n",
            "----------\n",
            "train Loss: 0.00035 Acc: 100.00000 macro-f1: 1.00000\n",
            "test Loss: 0.35003 Acc: 94.08100 macro-f1: 1.00000\n",
            "one epochs training time : 85m 25s\n",
            "\n",
            "\n",
            "Training complete in 85m 26s\n",
            "Best valid Acc: 34 - 94.8\n",
            "model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 결과 그래프 그리기\n",
        "print('best model : %d - %1.f / %.1f'%(best_idx, valid_acc[best_idx], valid_loss[best_idx]))\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "ax1.plot(train_acc, 'b-')\n",
        "ax1.plot(valid_acc, 'r-')\n",
        "plt.plot(best_idx, valid_acc[best_idx], 'ro')\n",
        "ax1.set_xlabel('epoch')\n",
        "# Make the y-axis label, ticks and tick labels match the line color.\n",
        "ax1.set_ylabel('acc', color='k')\n",
        "ax1.tick_params('y', colors='k')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(train_loss, 'g-')\n",
        "ax2.plot(valid_loss, 'k-')\n",
        "plt.plot(best_idx, valid_loss[best_idx], 'ro')\n",
        "ax2.set_ylabel('loss', color='k')\n",
        "ax2.tick_params('y', colors='k')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "5kJPLUuEU1pH",
        "outputId": "bb032d83-d89b-4923-f655-3db438b83bff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best model : 34 - 95 / 0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JJdS3MynU1rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 학습 결과 검수\n",
        "# def test_and_visualize_model(model, phase = 'test', num_images=4):\n",
        "#     # phase = 'train', 'valid', 'test'\n",
        "    \n",
        "#     was_training = model.training\n",
        "#     model.eval()\n",
        "#     fig = plt.figure()\n",
        "    \n",
        "#     running_loss, running_corrects, num_cnt = 0.0, 0, 0\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
        "#             inputs = inputs.to(device)\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#             outputs = model(inputs)\n",
        "#             _, preds = torch.max(outputs, 1)\n",
        "#             loss = criterion(outputs, labels)  # batch의 평균 loss 출력\n",
        "\n",
        "#             running_loss    += loss.item() * inputs.size(0)\n",
        "#             running_corrects+= torch.sum(preds == labels.data)\n",
        "#             num_cnt += inputs.size(0)  # batch size\n",
        "\n",
        "#     #         if i == 2: break\n",
        "\n",
        "#         test_loss = running_loss / num_cnt\n",
        "#         test_acc  = running_corrects.double() / num_cnt       \n",
        "#         print('test done : loss/acc : %.2f / %.1f' % (test_loss, test_acc*100))\n",
        "\n",
        "#     # 예시 그림 plot\n",
        "#     with torch.no_grad():\n",
        "#         for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
        "#             inputs = inputs.to(device)\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#             outputs = model(inputs)\n",
        "#             _, preds = torch.max(outputs, 1)        \n",
        "\n",
        "#             # 예시 그림 plot\n",
        "#             for j in range(1, num_images+1):\n",
        "#                 ax = plt.subplot(num_images//2, 2, j)\n",
        "#                 ax.axis('off')\n",
        "#                 ax.set_title('%s : %s -> %s'%(\n",
        "#                     'True' if class_names[str(labels[j].cpu().numpy())]==class_names[str(preds[j].cpu().numpy())] else 'False',\n",
        "#                     class_names[str(labels[j].cpu().numpy())], class_names[str(preds[j].cpu().numpy())]))\n",
        "#                 imshow(inputs.cpu().data[j])          \n",
        "#             if i == 0 : break\n",
        "\n",
        "\n",
        "#     model.train(mode=was_training);  # 다시 train모드로\n",
        "    \n",
        "#     ## TEST!\n",
        "#     test_and_visualize_model(model, phase = 'test')"
      ],
      "metadata": {
        "id": "bZK4jMWoU1tQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LedsQjpUU1vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X2jIgWqSU1w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-o1YRRNYU1zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 추론"
      ],
      "metadata": {
        "id": "WQR1RKgTpKk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 모델을 평가 모드로 변경하고 test 데이터 분류\n",
        "test_png = sorted(glob('/content/drive/Othercomputers/내 MacBook Pro/open/test/*.png'))\n",
        "test_imgs = [img_load(n) for n in tqdm(test_png)]\n",
        "test_dataset = Custom_dataset_3(np.array(test_imgs), np.array([\"tmp\"]*len(test_imgs)), mode='test')\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "model.eval()\n",
        "f_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in (test_loader):\n",
        "        x = torch.tensor(batch[0], dtype = torch.float32, device = device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            pred = model(x)\n",
        "        f_pred.extend(pred.argmax(1).detach().cpu().numpy().tolist())"
      ],
      "metadata": {
        "id": "gj7gTQv6U11P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9e166d5-53e0-4630-a79f-8b625eef3b33"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2154/2154 [03:11<00:00, 11.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 숫자로된 레이블을 문자열 레이블로 변경\n",
        "label_decoder = {value:key for key, value in label_unique.items()}\n",
        "f_result = [label_decoder[result] for result in f_pred]"
      ],
      "metadata": {
        "id": "30_rcMw4U13G"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime as dt \n",
        "today = dt.today().strftime('%Y-%m-%d')\n",
        "version = f'efficientNet_b4_by_timm_{today}'\n",
        "submission = pd.read_csv(f\"/content/drive/MyDrive/DACON_이상치 탐지 알고리즘 경진대회/open/sample_submission.csv\")\n",
        "\n",
        "## 시각적 확인을 위해 문자열 레이블로 이루어진 된 label 필드 생성\n",
        "submission[\"label\"] = f_result\n",
        "display(submission)\n",
        "submission.to_csv(f\"/content/drive/MyDrive/DACON_이상치 탐지 알고리즘 경진대회/open/sample_submission_{version}.csv\", index = False)"
      ],
      "metadata": {
        "id": "1zWu-D7WdRmQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "f588d0d2-9ff4-44f1-a3ac-fd251d0d3649"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      index             label\n",
              "0         0   tile-glue_strip\n",
              "1         1         grid-good\n",
              "2         2   transistor-good\n",
              "3         3        tile-rough\n",
              "4         4         tile-good\n",
              "...     ...               ...\n",
              "2149   2149  tile-gray_stroke\n",
              "2150   2150        screw-good\n",
              "2151   2151         grid-good\n",
              "2152   2152        cable-good\n",
              "2153   2153       zipper-good\n",
              "\n",
              "[2154 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-40e2176a-bd6c-4b27-942c-27797818f161\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>tile-glue_strip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>grid-good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>transistor-good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>tile-rough</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>tile-good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2149</th>\n",
              "      <td>2149</td>\n",
              "      <td>tile-gray_stroke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2150</th>\n",
              "      <td>2150</td>\n",
              "      <td>screw-good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2151</th>\n",
              "      <td>2151</td>\n",
              "      <td>grid-good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2152</th>\n",
              "      <td>2152</td>\n",
              "      <td>cable-good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2153</th>\n",
              "      <td>2153</td>\n",
              "      <td>zipper-good</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2154 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40e2176a-bd6c-4b27-942c-27797818f161')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-40e2176a-bd6c-4b27-942c-27797818f161 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-40e2176a-bd6c-4b27-942c-27797818f161');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3KcqXlWozhlc",
        "outputId": "14170aa4-1b93-45bd-e5c2-56d7556dc17e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'efficientNet_b4_by_timm_2022-05-04 16:01:31.212423'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt.today().strftime('%Y-%m-%d')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ts_l_I31zndi",
        "outputId": "5dea421a-ddc1-4580-ffd0-e3e5179e7ecc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2022-05-04'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cG_tNWs2zxh8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}